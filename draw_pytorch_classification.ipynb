{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.autograd as autograd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "T = 8\n",
    "batch_size = 100\n",
    "A = 60\n",
    "B = 60\n",
    "z_size = 10\n",
    "read_size = 12\n",
    "write_size = 12\n",
    "enc_size = 256\n",
    "dec_size = 256\n",
    "epoch_num = 50\n",
    "learning_rate = 1e-3\n",
    "beta1 = 0.5\n",
    "USE_CUDA = True\n",
    "clip = 5.0\n",
    "attention=1\n",
    "\n",
    "\n",
    "def Variable(data, *args, **kwargs):\n",
    "    if USE_CUDA:\n",
    "        data = data.cuda()\n",
    "    return autograd.Variable(data,*args, **kwargs)\n",
    "\n",
    "\n",
    "class DrawModel(nn.Module):\n",
    "    def __init__(self,T,A,B,z_size,read_size,write_size,dec_size,enc_size,attention):\n",
    "        super(DrawModel,self).__init__()\n",
    "        self.T = T\n",
    "        # self.batch_size = batch_size\n",
    "        self.attention = attention\n",
    "        self.A = A\n",
    "        self.B = B\n",
    "        self.z_size = z_size\n",
    "        self.read_size = read_size\n",
    "        self.write_size = write_size\n",
    "        self.dec_size = dec_size\n",
    "        self.enc_size = enc_size\n",
    "        self.cs = [0] * T #canvas\n",
    "        self.logsigmas,self.sigmas,self.mus = [0] * T,[0] * T,[0] * T\n",
    "\n",
    "        self.encoder = nn.LSTMCell(2 * B * A + dec_size, enc_size)   #encoder for read without attention\n",
    "        self.encoder_a = nn.LSTMCell(2 * read_size * read_size + dec_size, enc_size) #encoder for read with attention\n",
    "        self.decoder = nn.LSTMCell(z_size, dec_size)                 #decoder\n",
    "\n",
    "        self.mu_linear = nn.Linear(dec_size, z_size)    #linear for eq(1) for latent distribution\n",
    "        self.sigma_linear = nn.Linear(dec_size, z_size) #linear for eq(2) for latent distribution\n",
    "        self.dec_linear = nn.Linear(dec_size, 5)        #linear for eq(21) generating 5 params from h_dec\n",
    "        \n",
    "        self.dec_w_linear_a = nn.Linear(dec_size, write_size*write_size)     #linear for eq(28)\n",
    "        self.dec_w_linear = nn.Linear(dec_size, A*B)                         #linear for eq(18)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(enc_size, 10)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        self.batch_size = x.size()[0]\n",
    "        h_enc_prev = Variable(torch.zeros(self.batch_size, self.enc_size))\n",
    "        h_dec_prev = Variable(torch.zeros(self.batch_size, self.dec_size))\n",
    "\n",
    "        enc_state = Variable(torch.zeros(self.batch_size,self.enc_size))\n",
    "        dec_state = Variable(torch.zeros(self.batch_size, self.dec_size))\n",
    "\n",
    "        if (self.attention):\n",
    "            for t in range(self.T):\n",
    "              c_prev = Variable(torch.zeros(self.batch_size,self.A * self.B)) if t == 0 else self.cs[t-1]\n",
    "              x_hat = x - self.sigmoid(c_prev)     #eq(3)\n",
    "              r_t = self.read(x,x_hat,h_dec_prev,1)  #eq(4)\n",
    "              h_enc, enc_state = self.encoder_a(torch.cat((r_t,h_dec_prev),1),(h_enc_prev,enc_state)) #eq(5)\n",
    "              h_enc_prev = h_enc\n",
    "              z, self.mus[t], self.logsigmas[t], self.sigmas[t] = self.sampleQ(h_enc) #eq(6)\n",
    "              h_dec, dec_state = self.decoder(z, (h_dec_prev, dec_state))\n",
    "              self.cs[t] = c_prev + self.write(h_dec,1)\n",
    "              h_dec_prev = h_dec\n",
    "            \n",
    "            probs = torch.nn.functional.log_softmax(self.fc(h_enc), dim=1)\n",
    "            #print(probs.view(batch_size,10))\n",
    "            return probs.view(batch_size,10)\n",
    "\n",
    "\n",
    "    def gaussian_filter(self,h_dec,N):\n",
    "        params = self.dec_linear(h_dec) #eq(21)\n",
    "        gx, gy, log_sigma2, log_delta, log_gamma = torch.split(params, 1, dim=1) #each param has size batch_sizex1\n",
    "        gx = (self.A+1)/2 * (gx+1) #eq(22)\n",
    "        gy = (self.B+1)/2 * (gy+1) #eq(23)\n",
    "        delta = (max(self.A,self.B) - 1) / (N - 1) * torch.exp(log_delta)  #eq(24)\n",
    "        sigma2 = torch.exp(log_sigma2) \n",
    "        gamma = torch.exp(log_gamma)\n",
    "        \n",
    "        #make mean_x, mean_y, a, b into tensors with shape (1, N, AorB) to make Fx and Fy with the same shape\n",
    "        mean_x = torch.zeros((batch_size,A)).to('cuda:0') #use this trick because I cannot make empty tensor \n",
    "        mean_y = torch.zeros((batch_size,B)).to('cuda:0') #use this trick because I cannot make empty tensor\n",
    "        for i in range (N):\n",
    "          mean_x_i = torch.ones((batch_size,A)).to('cuda:0') * (gx + (i- N/2 -0.5)*delta)  #eq(19) \n",
    "          #print(mean_x.shape,mean_x_i.shape)\n",
    "          mean_x = torch.cat([mean_x, mean_x_i], axis=-1)\n",
    "          mean_y_j = torch.ones((batch_size,A)).to('cuda:0') * (gy + (i- N/2 -0.5)*delta)  #eq(20) \n",
    "          mean_y = torch.cat([mean_y,mean_y_j], axis =-1)\n",
    "        mean_x = mean_x[:,A:]     #undo the torch.zeros trick\n",
    "        mean_y = mean_y[:,B:]     #undo the torch.zeros trick\n",
    "        mean_x = mean_x.view(batch_size, N, A) #each row consists of A elements of mean_x(i)\n",
    "        mean_y = mean_y.view(batch_size, N, B) #each row consists of B elements of mean_y(j)\n",
    "        a = torch.zeros((delta.size()[0],A)).to('cuda:0')\n",
    "        b = torch.zeros((delta.size()[0],B)).to('cuda:0')\n",
    "        for i in range (N):\n",
    "          ai = torch.ones((delta.size()[0],A)).to('cuda:0') * torch.range(0,A-1).to('cuda:0')\n",
    "          bj = torch.ones((delta.size()[0],B)).to('cuda:0') * torch.range(0,B-1).to('cuda:0')\n",
    "          a = torch.cat([a,ai], axis=-1)\n",
    "          b = torch.cat([b,bj], axis=-1)\n",
    "        a = a[:,A:]\n",
    "        b = b[:,B:]\n",
    "        a = a.view(batch_size, N, A)\n",
    "        b = b.view(batch_size, N, B)\n",
    "\n",
    "        sig = torch.ones(A).to('cuda:0') * sigma2\n",
    "        sigma_2 = sig\n",
    "        for i in range(N-1):\n",
    "          sigma_2 = torch.cat([sigma_2,sig], axis=1)\n",
    "        sigma_2 = sigma_2.view(-1,N,A)\n",
    "        #print(a.size(),mean_x.size(),sigma_2.size())\n",
    "        Fx = torch.exp(-torch.square(a-mean_x)/(2*sigma_2))  #eq(25)\n",
    "        Fy = torch.exp(-torch.square(b-mean_y)/(2*sigma_2))  #eq(26)\n",
    "        #normalize each row of Fx and Fy to sum 1\n",
    "        Fx=Fx/(torch.sum(Fx,2,keepdim=True)+1e-8)\n",
    "        Fy=Fy/(torch.sum(Fy,2,keepdim=True)+1e-8)\n",
    "        \n",
    "\n",
    "        return Fx,Fy,gamma\n",
    "\n",
    "    def read(self,x,x_hat,h_dec_prev, attention):\n",
    "        if (attention):\n",
    "          Fx,Fy,gamma = self.gaussian_filter(h_dec_prev, self.read_size)\n",
    "          #transpose Fx per batch\n",
    "          Fxt = Fx.transpose(2,1)\n",
    "          #reshape x from (batch,B*A) to (batch,B,A)\n",
    "          x_matrix = x.view(-1,B,A)\n",
    "          xhat_matrix = x_hat.view(-1,B,A)\n",
    "          #concat\n",
    "          first_half = torch.bmm(torch.bmm(Fy,x_matrix),Fxt)\n",
    "          first_half = first_half.view(-1,self.read_size*self.read_size)\n",
    "          second_half = torch.bmm(torch.bmm(Fy,xhat_matrix),Fxt)\n",
    "          second_half = first_half.view(-1,self.read_size*self.read_size)\n",
    "          gamma = gamma.view(-1,1)\n",
    "          return gamma*(torch.cat([first_half,second_half], axis=1)) #eq(27)\n",
    "        else:\n",
    "          return torch.cat([x,x_hat], 1)  #eq(17)\n",
    "            \n",
    "    def write(self,h_dec, attention):\n",
    "        if (attention):\n",
    "          w = self.dec_w_linear_a(h_dec) #eq(28)\n",
    "          w = w.view(self.batch_size,self.write_size,self.write_size)\n",
    "          Fx,Fy,gamma = self.gaussian_filter(h_dec,write_size)\n",
    "          Fyt = Fy.transpose(2,1)\n",
    "          wr =  torch.bmm(torch.bmm(Fyt,w),Fx)      #eq(29)\n",
    "          wr = wr.view(self.batch_size,self.A*self.B)\n",
    "          return wr / gamma.view(-1,1)\n",
    "        else:\n",
    "          return self.dec_w_linear(h_dec) #eq(18)\n",
    "\n",
    "    def sampleQ(self,h_enc):\n",
    "        noise = Variable(torch.randn(self.batch_size, self.z_size))\n",
    "        mu = self.mu_linear(h_enc)           #eq(1)\n",
    "        log_sigma = self.sigma_linear(h_enc) #eq(2)\n",
    "        sigma = torch.exp(log_sigma)\n",
    "\n",
    "        return mu + sigma * noise , mu , log_sigma, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ny_test_onehot = []\\nfor i in range(y_test.shape[0]):\\n    basic = np.zeros(10)\\n    basic[y_test[i][0]] = 1\\n    y_test_onehot = np.append(y_test_onehot,basic)\\ny_test_onehot = y_test_onehot.reshape(y_test.shape[0],-1)\\nprint(y_test_onehot.shape)\\n\\n\\n#plt.imshow(x_train[8], interpolation='nearest')\\n#plt.show()\\n\\nnp.save('CMNIST_ytrain',y_train_onehot)\\nnp.save('CMNIST_yval',y_val_onehot)\\nnp.save('CMNIST_ytest',y_test_onehot)\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cluttered_MNIST (60X60)\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = loadmat('/Users/bellagodiva/Downloads/cluttered-mnist.mat')\n",
    "x_train = data['x_tr']\n",
    "x_train = np.transpose(x_train, (3,0,1,2))\n",
    "x_train = x_train.reshape(-1,60,60)\n",
    "y_train = data['y_tr']\n",
    "y_train = np.transpose(y_train)\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_train_onehot = []\n",
    "\n",
    "x_val = data['x_vl']\n",
    "x_val = np.transpose(x_val, (3,0,1,2))\n",
    "x_val = x_val.reshape(-1,60,60)\n",
    "y_val = data['y_vl']\n",
    "y_val = np.transpose(y_val)\n",
    "y_val = y_val.reshape(-1,1)\n",
    "\n",
    "x_test = data['x_ts']\n",
    "x_test = np.transpose(x_test, (3,0,1,2))\n",
    "x_test = x_test.reshape(-1,60,60)\n",
    "y_test = data['y_ts']\n",
    "y_test = np.transpose(y_test)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "'''\n",
    "y_test_onehot = []\n",
    "for i in range(y_test.shape[0]):\n",
    "    basic = np.zeros(10)\n",
    "    basic[y_test[i][0]] = 1\n",
    "    y_test_onehot = np.append(y_test_onehot,basic)\n",
    "y_test_onehot = y_test_onehot.reshape(y_test.shape[0],-1)\n",
    "print(y_test_onehot.shape)\n",
    "\n",
    "\n",
    "#plt.imshow(x_train[8], interpolation='nearest')\n",
    "#plt.show()\n",
    "\n",
    "np.save('CMNIST_ytrain',y_train_onehot)\n",
    "np.save('CMNIST_yval',y_val_onehot)\n",
    "np.save('CMNIST_ytest',y_test_onehot)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "#x_train = 255 * x_train\n",
    "#x_train = x_train.astype(np.uint8)\n",
    "#x_val = 255 * x_val\n",
    "#x_val = x_val.astype(np.uint8)\n",
    "\n",
    "#plt.imshow(x_train[8], interpolation='nearest')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        self.data = data\n",
    "        self.targets = torch.LongTensor(targets)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        x = self.transform(x)\n",
    "        y = self.targets[index]\n",
    "        return x,y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "train_dataset = MyDataset(x_train[:20000],y_train[:20000], transform=transforms.Compose([transforms.ToTensor()]))\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True )\n",
    "val_dataset = MyDataset(x_val[:3000],y_val[:3000], transform=transforms.Compose([transforms.ToTensor()]))\n",
    "val_loader = DataLoader(val_dataset, batch_size=100, shuffle=True )\n",
    "test_dataset = MyDataset(x_test[:3000],y_test[:3000], transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-be383057d346>:112: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  ai = torch.ones((delta.size()[0],A)).to('cuda:0') * torch.range(0,A-1).to('cuda:0')\n",
      "<ipython-input-52-be383057d346>:113: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  bj = torch.ones((delta.size()[0],B)).to('cuda:0') * torch.range(0,B-1).to('cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t\t Training Loss: 2.302756202220917 \t\t Validation Loss: 7.665491898854573\n",
      "Epoch 2 \t\t Training Loss: 2.3014004385471343 \t\t Validation Loss: 7.6585618654886884\n",
      "Epoch 3 \t\t Training Loss: 2.300270003080368 \t\t Validation Loss: 7.622946898142497\n",
      "Epoch 4 \t\t Training Loss: 2.299338111877441 \t\t Validation Loss: 7.6664360364278155\n",
      "Epoch 5 \t\t Training Loss: 2.298586632013321 \t\t Validation Loss: 7.65940507253011\n",
      "Epoch 6 \t\t Training Loss: 2.2976794731616974 \t\t Validation Loss: 7.644294897715251\n",
      "Epoch 7 \t\t Training Loss: 2.296746028661728 \t\t Validation Loss: 7.61693795522054\n",
      "Epoch 8 \t\t Training Loss: 2.2959323990345 \t\t Validation Loss: 7.665892442067464\n",
      "Epoch 9 \t\t Training Loss: 2.2948017501831055 \t\t Validation Loss: 7.643311818440755\n",
      "Epoch 10 \t\t Training Loss: 2.293797458410263 \t\t Validation Loss: 7.62001911799113\n",
      "Epoch 11 \t\t Training Loss: 2.2926428055763246 \t\t Validation Loss: 7.590138117472331\n",
      "Epoch 12 \t\t Training Loss: 2.291331737041473 \t\t Validation Loss: 7.630549271901448\n",
      "Epoch 13 \t\t Training Loss: 2.2898990607261656 \t\t Validation Loss: 7.617808183034261\n",
      "Epoch 14 \t\t Training Loss: 2.288337962627411 \t\t Validation Loss: 7.607962290445964\n",
      "Epoch 15 \t\t Training Loss: 2.2865901863574982 \t\t Validation Loss: 7.596367200215657\n",
      "Epoch 16 \t\t Training Loss: 2.284608427286148 \t\t Validation Loss: 7.590208848317464\n",
      "Epoch 17 \t\t Training Loss: 2.2824499297142027 \t\t Validation Loss: 7.587024370829265\n",
      "Epoch 18 \t\t Training Loss: 2.280108243227005 \t\t Validation Loss: 7.58311112721761\n",
      "Epoch 19 \t\t Training Loss: 2.277709087133408 \t\t Validation Loss: 7.603387832641602\n",
      "Epoch 20 \t\t Training Loss: 2.2748712730407714 \t\t Validation Loss: 7.542482217152913\n",
      "Epoch 21 \t\t Training Loss: 2.2717946672439577 \t\t Validation Loss: 7.560853958129883\n",
      "Epoch 22 \t\t Training Loss: 2.26830153465271 \t\t Validation Loss: 7.497328917185466\n",
      "Epoch 23 \t\t Training Loss: 2.264720070362091 \t\t Validation Loss: 7.569958368937175\n",
      "Epoch 24 \t\t Training Loss: 2.2607047855854034 \t\t Validation Loss: 7.547287940979004\n",
      "Epoch 25 \t\t Training Loss: 2.25641716837883 \t\t Validation Loss: 7.532289822896321\n",
      "Epoch 26 \t\t Training Loss: 2.251916425228119 \t\t Validation Loss: 7.540829181671143\n",
      "Epoch 27 \t\t Training Loss: 2.2467371129989626 \t\t Validation Loss: 7.448299725850423\n",
      "Epoch 28 \t\t Training Loss: 2.2416752755641935 \t\t Validation Loss: 7.553101380666097\n",
      "Epoch 29 \t\t Training Loss: 2.235830465555191 \t\t Validation Loss: 7.423654397328694\n",
      "Epoch 30 \t\t Training Loss: 2.2295239615440368 \t\t Validation Loss: 7.488439083099365\n",
      "Epoch 31 \t\t Training Loss: 2.222457001209259 \t\t Validation Loss: 7.335520585378011\n",
      "Epoch 32 \t\t Training Loss: 2.2154237985610963 \t\t Validation Loss: 7.2655073801676435\n",
      "Epoch 33 \t\t Training Loss: 2.2080854761600492 \t\t Validation Loss: 7.449234326680501\n",
      "Epoch 34 \t\t Training Loss: 2.200634114742279 \t\t Validation Loss: 7.4208879470825195\n",
      "Epoch 35 \t\t Training Loss: 2.1928633189201356 \t\t Validation Loss: 7.379218737284343\n",
      "Epoch 36 \t\t Training Loss: 2.184604411125183 \t\t Validation Loss: 7.139999866485596\n",
      "Epoch 37 \t\t Training Loss: 2.1786427628993987 \t\t Validation Loss: 7.03826109568278\n",
      "Epoch 38 \t\t Training Loss: 2.171968870162964 \t\t Validation Loss: 7.251533667246501\n",
      "Epoch 39 \t\t Training Loss: 2.1658170306682587 \t\t Validation Loss: 7.283289432525635\n",
      "Epoch 40 \t\t Training Loss: 2.1616968655586244 \t\t Validation Loss: 7.215474446614583\n",
      "Epoch 41 \t\t Training Loss: 2.1584204852581026 \t\t Validation Loss: 7.4105485280354815\n",
      "Epoch 42 \t\t Training Loss: 2.152760841846466 \t\t Validation Loss: 7.357405821482341\n",
      "Epoch 43 \t\t Training Loss: 2.1502771401405334 \t\t Validation Loss: 7.399555842081706\n",
      "Epoch 44 \t\t Training Loss: 2.144844836592674 \t\t Validation Loss: 7.087835470835368\n",
      "Epoch 45 \t\t Training Loss: 2.139545801281929 \t\t Validation Loss: 7.3781561851501465\n",
      "Epoch 46 \t\t Training Loss: 2.134523861408234 \t\t Validation Loss: 7.595843474070231\n",
      "Epoch 47 \t\t Training Loss: 2.1274484783411025 \t\t Validation Loss: 7.001629670461019\n",
      "Epoch 48 \t\t Training Loss: 2.1202082163095475 \t\t Validation Loss: 7.2932664553324384\n",
      "Epoch 49 \t\t Training Loss: 2.1106819766759872 \t\t Validation Loss: 6.787922382354736\n",
      "Epoch 50 \t\t Training Loss: 2.097898497581482 \t\t Validation Loss: 6.882350444793701\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "model = DrawModel(T,A,B,z_size,read_size,write_size,dec_size,enc_size,attention)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=learning_rate,betas=(beta1,0.999))\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model.to('cuda:0')\n",
    "    \n",
    "min_valid_loss = np.inf\n",
    "\n",
    "for e in range(epoch_num):\n",
    "    train_loss = 0.0\n",
    "    for data, labels in train_loader:\n",
    "        bs = data.size()[0]\n",
    "        data = data.view(bs, -1)\n",
    "        labels = torch.squeeze(labels)\n",
    "        if torch.cuda.is_available():\n",
    "            data, labels = data.cuda(), labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        target = model(data)\n",
    "        loss = criterion(target,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    valid_loss = 0.0\n",
    "    for data, labels in val_loader:\n",
    "        bs = data.size()[0]\n",
    "        data = data.view(bs, -1)\n",
    "        labels = torch.squeeze(labels)\n",
    "        if torch.cuda.is_available():\n",
    "            data, labels = data.cuda(), labels.cuda()\n",
    "        \n",
    "        target = model(data)\n",
    "        loss = criterion(target,labels)\n",
    "        valid_loss = loss.item() * data.size(0)\n",
    "    \n",
    "    print(f'Epoch {e+1} \\t\\t Training Loss: {train_loss / len(train_loader)} \\t\\t Validation Loss: {valid_loss / len(val_loader)}')\n",
    "    if min_valid_loss > valid_loss:\n",
    "        #print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
    "        min_valid_loss = valid_loss\n",
    "        # Saving State Dict\n",
    "        torch.save(model.state_dict(), '/Users/ivy2021/Documents/DRAW/class_model/saved_model.pth')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-be383057d346>:112: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  ai = torch.ones((delta.size()[0],A)).to('cuda:0') * torch.range(0,A-1).to('cuda:0')\n",
      "<ipython-input-52-be383057d346>:113: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  bj = torch.ones((delta.size()[0],B)).to('cuda:0') * torch.range(0,B-1).to('cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 21 %\n"
     ]
    }
   ],
   "source": [
    "model_test = DrawModel(T,A,B,z_size,read_size,write_size,dec_size,enc_size,attention)\n",
    "model_test.cuda()\n",
    "model_test.load_state_dict(torch.load('/Users/ivy2021/Documents/DRAW/class_model/saved_model.pth'))\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        bs = data.size()[0]\n",
    "        data = data.view(bs, -1)\n",
    "        labels = torch.squeeze(labels)\n",
    "        if torch.cuda.is_available():\n",
    "            data, labels = data.cuda(), labels.cuda()\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = model_test(data)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predicted.cuda()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1, 60, 60)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(6 * 28 * 28, 265)\n",
    "        self.fc2 = nn.Linear(265, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "#cluttered_MNIST (60X60)\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = loadmat('/Users/bellagodiva/Downloads/cluttered-mnist.mat')\n",
    "x_train = data['x_tr']\n",
    "x_train = np.transpose(x_train, (3,2,0,1))\n",
    "x_train = x_train.reshape(-1,1,60,60)\n",
    "y_train = data['y_tr']\n",
    "y_train = np.transpose(y_train)\n",
    "y_train = y_train.reshape(-1,1)\n",
    "\n",
    "\n",
    "x_val = data['x_vl']\n",
    "x_val = np.transpose(x_val, (3,2,0,1))\n",
    "x_val = x_val.reshape(-1,1,60,60)\n",
    "y_val = data['y_vl']\n",
    "y_val = np.transpose(y_val)\n",
    "y_val = y_val.reshape(-1,1)\n",
    "\n",
    "x_test = data['x_ts']\n",
    "x_test = np.transpose(x_test, (3,2,0,1))\n",
    "x_test = x_test.reshape(-1,1,60,60)\n",
    "y_test = data['y_ts']\n",
    "y_test = np.transpose(y_test)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "#x_train = 255 * x_train\n",
    "#x_train = x_train.astype(np.uint8)\n",
    "#x_val = 255 * x_val\n",
    "#x_val = x_val.astype(np.uint8)\n",
    "\n",
    "#plt.imshow(x_train[8], interpolation='nearest')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        self.data = data\n",
    "        self.targets = torch.LongTensor(targets)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        x = self.transform(x)\n",
    "        y = self.targets[index]\n",
    "        return x,y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "train_dataset = MyDataset(x_train,y_train, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True )\n",
    "val_dataset = MyDataset(x_val,y_val, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "val_loader = DataLoader(val_dataset, batch_size=100, shuffle=True )\n",
    "test_dataset = MyDataset(x_test,y_test, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t\t Training Loss: 2.263202792644501 \t\t Validation Loss: 2.261820077896118\n",
      "Epoch 2 \t\t Training Loss: 2.1896858344078063 \t\t Validation Loss: 2.2197563648223877\n",
      "Epoch 3 \t\t Training Loss: 2.1505018973350527 \t\t Validation Loss: 2.1956377029418945\n",
      "Epoch 4 \t\t Training Loss: 2.1313251876831054 \t\t Validation Loss: 2.23964786529541\n",
      "Epoch 5 \t\t Training Loss: 2.1091938989162444 \t\t Validation Loss: 2.053295850753784\n",
      "Epoch 6 \t\t Training Loss: 2.078376898288727 \t\t Validation Loss: 2.007622480392456\n",
      "Epoch 7 \t\t Training Loss: 2.0328746910095217 \t\t Validation Loss: 2.04443097114563\n",
      "Epoch 8 \t\t Training Loss: 1.9677379574775695 \t\t Validation Loss: 1.9945449829101562\n",
      "Epoch 9 \t\t Training Loss: 1.8766384677886963 \t\t Validation Loss: 1.840621829032898\n",
      "Epoch 10 \t\t Training Loss: 1.7647279064655303 \t\t Validation Loss: 1.795608639717102\n",
      "Epoch 11 \t\t Training Loss: 1.6493438987731934 \t\t Validation Loss: 1.7827885150909424\n",
      "Epoch 12 \t\t Training Loss: 1.535998100757599 \t\t Validation Loss: 1.6181226968765259\n",
      "Epoch 13 \t\t Training Loss: 1.424509726047516 \t\t Validation Loss: 1.5976104736328125\n",
      "Epoch 14 \t\t Training Loss: 1.3202546336650849 \t\t Validation Loss: 1.4594063758850098\n",
      "Epoch 15 \t\t Training Loss: 1.2271665536165237 \t\t Validation Loss: 1.515579104423523\n",
      "Epoch 16 \t\t Training Loss: 1.1460695254802704 \t\t Validation Loss: 1.214753270149231\n",
      "Epoch 17 \t\t Training Loss: 1.074316936135292 \t\t Validation Loss: 1.367864966392517\n",
      "Epoch 18 \t\t Training Loss: 1.0062205613851547 \t\t Validation Loss: 1.3864479064941406\n",
      "Epoch 19 \t\t Training Loss: 0.9457804188728333 \t\t Validation Loss: 1.3578588962554932\n",
      "Epoch 20 \t\t Training Loss: 0.8917587084770202 \t\t Validation Loss: 1.3985710144042969\n"
     ]
    }
   ],
   "source": [
    "min_valid_loss = np.inf\n",
    "\n",
    "for e in range(20):\n",
    "    train_loss = 0.0\n",
    "    for data, labels in train_loader:\n",
    "        bs = data.size()[0]\n",
    "        data = data.view(bs, 1,60,60)\n",
    "        labels = torch.squeeze(labels)\n",
    "        #labels = labels.view(bs,1)\n",
    "        if torch.cuda.is_available():\n",
    "            data, labels = data.cuda(), labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        target = model(data)\n",
    "        loss = criterion(target,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    valid_loss = 0.0\n",
    "    for data, labels in val_loader:\n",
    "        bs = data.size()[0]\n",
    "        data = data.view(bs, 1,60,60)\n",
    "        labels = torch.squeeze(labels)\n",
    "        #labels = labels.view(bs,1)\n",
    "        if torch.cuda.is_available():\n",
    "            data, labels = data.cuda(), labels.cuda()\n",
    "        \n",
    "        target = model(data)\n",
    "        loss = criterion(target,labels)\n",
    "        valid_loss = loss.item() * data.size(0)\n",
    "    \n",
    "    print(f'Epoch {e+1} \\t\\t Training Loss: {train_loss / len(train_loader)} \\t\\t Validation Loss: {valid_loss / len(val_loader)}')\n",
    "    if min_valid_loss > valid_loss:\n",
    "        #print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
    "        min_valid_loss = valid_loss\n",
    "        # Saving State Dict\n",
    "        torch.save(model.state_dict(), '/Users/bellagodiva/Downloads/DRAW/class_model/saved_model1.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 53 %\n"
     ]
    }
   ],
   "source": [
    "model_test = Net()\n",
    "model_test.load_state_dict(torch.load('/Users/bellagodiva/Downloads/DRAW/class_model/saved_model1.pth'))\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        bs = data.size()[0]\n",
    "        data = data.view(bs, 1,60,60)\n",
    "        labels = torch.squeeze(labels)\n",
    "        if torch.cuda.is_available():\n",
    "            data, labels = data.cuda(), labels.cuda()\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = model_test(data)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
